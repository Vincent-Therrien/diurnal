{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNA Secondary Structure Prediction Pipelines\n",
    "\n",
    "This notebook predicts RNA secondary structure using pipelines composed of the\n",
    "following units:\n",
    "\n",
    "- Global structure estimator (GSE)\n",
    "- Local structure refiner (LSR)\n",
    "- Monomialization unit (MU)\n",
    "- Binarization unit (BU)\n",
    "- Symmetrization unit (SU)\n",
    "- Constraint unit (CU)\n",
    "- Threshold unit (TU)\n",
    "- Noise removal unit (NRU)\n",
    "- Output unit: SU -> CU -> MU -> BU -> NRU (OU)\n",
    "\n",
    "The last pipeline components are always OU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from random import sample\n",
    "\n",
    "import numpy as np\n",
    "from torch import optim, nn\n",
    "\n",
    "from diurnal import database, structure, train, transform, visualize, evaluate, segment\n",
    "from diurnal.models import deep\n",
    "from diurnal.models.deep import cnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-11T21:25:44.603842 > INFO Download and install an RNA database.\n",
      "2024-06-11T21:25:44.638979 >     The dataset `archiveII` is already downloaded at `./data/archiveII`.\n",
      "2024-06-11T21:25:44.641005 > INFO Extract the filenames from the directory `./data/archiveII/`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3975/3975 [00:32<00:00, 123.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-11T21:26:16.880722 >     Detected 3975 files. Kept 2326 files.\n",
      "2024-06-11T21:26:16.909745 >     The file `./data/archiveII_processed_128/names.txt` already contains the names.\n"
     ]
    }
   ],
   "source": [
    "SIZE = 128  # RNA molecule maximum length (longer ones are filtered out).\n",
    "SUBDIVISION_SIZE = 32  # Kernel size of the local structure refiner.\n",
    "\n",
    "database.download(\"./data/\", \"archiveII\")\n",
    "SRC = \"./data/archiveII/\"  # Input directory containing the CT files.\n",
    "DST = f\"./data/archiveII_processed_{SIZE}/\"  # Output directory of the formatted files.\n",
    "names = database.format_filenames(SRC, DST + \"names.txt\", SIZE)\n",
    "train_names, validation_names, test_names = train.split(\n",
    "    names, (0.8, 0.1, 0.1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-11T21:26:16.966183 > INFO Formatting primary structures into `./data/archiveII_processed_128/validation/primary_potential_pairings_scalar.npy`.\n",
      "2024-06-11T21:26:17.110368 >     The file `./data/archiveII_processed_128/validation/primary_potential_pairings_scalar.npy` already contains the formatted data.\n",
      "2024-06-11T21:26:17.113394 > INFO Formatting primary structures into `./data/archiveII_processed_128/validation/primary_masks.npy`.\n",
      "2024-06-11T21:26:18.059901 >     The file `./data/archiveII_processed_128/validation/primary_masks.npy` already contains the formatted data.\n",
      "2024-06-11T21:26:18.062634 > INFO Formatting primary structures into `./data/archiveII_processed_128/validation/primary_onehot.npy`.\n",
      "2024-06-11T21:26:18.787731 >     The file `./data/archiveII_processed_128/validation/primary_onehot.npy` already contains the formatted data.\n",
      "2024-06-11T21:26:18.790510 > INFO Formatting secondary structures into `./data/archiveII_processed_128/validation/secondary_contact.npy`.\n",
      "2024-06-11T21:26:18.926422 >     The file `./data/archiveII_processed_128/validation/secondary_contact.npy` already contains the formatted data.\n",
      "2024-06-11T21:26:18.953066 > INFO Formatting primary structures into `./data/archiveII_processed_128/test/primary_potential_pairings_scalar.npy`.\n",
      "2024-06-11T21:26:19.587531 >     The file `./data/archiveII_processed_128/test/primary_potential_pairings_scalar.npy` already contains the formatted data.\n",
      "2024-06-11T21:26:19.591124 > INFO Formatting primary structures into `./data/archiveII_processed_128/test/primary_masks.npy`.\n",
      "2024-06-11T21:26:20.543441 >     The file `./data/archiveII_processed_128/test/primary_masks.npy` already contains the formatted data.\n",
      "2024-06-11T21:26:20.546599 > INFO Formatting primary structures into `./data/archiveII_processed_128/test/primary_onehot.npy`.\n",
      "2024-06-11T21:26:20.574730 >     The file `./data/archiveII_processed_128/test/primary_onehot.npy` already contains the formatted data.\n",
      "2024-06-11T21:26:20.577731 > INFO Formatting secondary structures into `./data/archiveII_processed_128/test/secondary_contact.npy`.\n",
      "2024-06-11T21:26:20.606131 >     The file `./data/archiveII_processed_128/test/secondary_contact.npy` already contains the formatted data.\n",
      "2024-06-11T21:26:20.625958 > INFO Formatting primary structures into `./data/archiveII_processed_128/train/primary_potential_pairings_scalar.npy`.\n",
      "2024-06-11T21:26:20.675974 >     The file `./data/archiveII_processed_128/train/primary_potential_pairings_scalar.npy` already contains the formatted data.\n",
      "2024-06-11T21:26:20.679490 > INFO Formatting primary structures into `./data/archiveII_processed_128/train/primary_masks.npy`.\n",
      "2024-06-11T21:26:20.762786 >     The file `./data/archiveII_processed_128/train/primary_masks.npy` already contains the formatted data.\n",
      "2024-06-11T21:26:20.766788 > INFO Formatting primary structures into `./data/archiveII_processed_128/train/primary_onehot.npy`.\n",
      "2024-06-11T21:26:20.794086 >     The file `./data/archiveII_processed_128/train/primary_onehot.npy` already contains the formatted data.\n",
      "2024-06-11T21:26:20.797403 > INFO Formatting secondary structures into `./data/archiveII_processed_128/train/secondary_contact.npy`.\n",
      "2024-06-11T21:26:20.840328 >     The file `./data/archiveII_processed_128/train/secondary_contact.npy` already contains the formatted data.\n"
     ]
    }
   ],
   "source": [
    "def format(dst: str, names: list[str]):\n",
    "    # Primary structures\n",
    "    pp_scalar = lambda x, y: structure.Primary.to_matrix(\n",
    "        x, y, structure.Schemes.IUPAC_PAIRINGS_SCALARS\n",
    "    )\n",
    "    database.format_primary_structure(\n",
    "        names, f\"{dst}primary_potential_pairings_scalar.npy\", SIZE, pp_scalar\n",
    "    )\n",
    "    database.format_primary_structure(\n",
    "        names, f\"{dst}primary_masks.npy\", SIZE, structure.Primary.to_mask\n",
    "    )\n",
    "    database.format_primary_structure(\n",
    "        names, f\"{dst}primary_onehot.npy\",\n",
    "        SIZE, structure.Primary.to_onehot\n",
    "    )\n",
    "    # Secondary structures.\n",
    "    database.format_secondary_structure(\n",
    "        names, f\"{dst}secondary_contact.npy\", SIZE,\n",
    "        structure.Secondary.to_matrix\n",
    "    )\n",
    "\n",
    "format(f\"{DST}validation/\", validation_names)\n",
    "format(f\"{DST}test/\", test_names)\n",
    "format(f\"{DST}train/\", train_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_data() -> tuple:\n",
    "    train_set = {\n",
    "        \"input\": (np.load(f\"{DST}train/primary_potential_pairings_scalar.npy\"), ),\n",
    "        \"output\": np.load(f\"{DST}train/secondary_contact.npy\"),\n",
    "        \"mask\": np.load(f\"{DST}train/primary_masks.npy\"),\n",
    "        \"names\": []\n",
    "    }\n",
    "    validation_set = {\n",
    "        \"input\": (np.load(f\"{DST}validation/primary_potential_pairings_scalar.npy\"), ),\n",
    "        \"output\": np.load(f\"{DST}validation/secondary_contact.npy\"),\n",
    "        \"mask\": np.load(f\"{DST}validation/primary_masks.npy\"),\n",
    "        \"names\": []\n",
    "    }\n",
    "    test_set = {\n",
    "        \"input\": (np.load(f\"{DST}test/primary_potential_pairings_scalar.npy\"), ),\n",
    "        \"output\": np.load(f\"{DST}test/secondary_contact.npy\"),\n",
    "        \"mask\": np.load(f\"{DST}test/primary_masks.npy\"),\n",
    "        \"names\": []\n",
    "    }\n",
    "    return train_set, validation_set, test_set\n",
    "\n",
    "\n",
    "# Create a training set for the LSR.\n",
    "from os import mkdir\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def save_global_estimations(model, data, name):\n",
    "    try:\n",
    "        mkdir(\"tmp\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    predictions = []\n",
    "    for i in tqdm(range(len(data[\"input\"][0]))):\n",
    "        p = model.predict([data[\"input\"][0][i]])\n",
    "        predictions.append(p)\n",
    "    np.save(f\"tmp/{name}\", np.array(predictions))\n",
    "\n",
    "\n",
    "FIRST = 16  # Select the most important areas.\n",
    "RANDOM = 8  # Select random areas after the FIRST ones.\n",
    "STRIDE = 16\n",
    "\n",
    "\n",
    "def sample_crops(predictions, data, name, maximum, t = 0) -> list:\n",
    "    predict_crops = []\n",
    "    pp_crops = []  # Potential pairings\n",
    "    contact_crops = []\n",
    "    for index in tqdm(range(len(predictions))):\n",
    "        prediction = predictions[index] * predictions[index].T * data[\"mask\"][index]\n",
    "        all_crops = segment.sample_areas(\n",
    "            prediction, SUBDIVISION_SIZE, stride=STRIDE, threshold=t\n",
    "        )\n",
    "        if len(all_crops) > FIRST:\n",
    "            difference = len(all_crops) - FIRST\n",
    "            sampling = min(difference, RANDOM)\n",
    "            if sampling:\n",
    "                crops = all_crops[:FIRST] + sample(all_crops[FIRST:], sampling)\n",
    "            else:\n",
    "                crops = all_crops[:FIRST]\n",
    "        else:\n",
    "            crops = all_crops\n",
    "\n",
    "        for crop, _ in crops:\n",
    "            predict_crops.append(\n",
    "                predictions[index][crop[0]: crop[0] + 32, crop[1]: crop[1] + 32]\n",
    "            )\n",
    "            pp_crops.append(\n",
    "                data[\"input\"][0][index][crop[0]: crop[0] + 32, crop[1]: crop[1] + 32]\n",
    "            )\n",
    "            contact_crops.append(\n",
    "                data[\"output\"][index][crop[0]: crop[0] + 32, crop[1]: crop[1] + 32]\n",
    "            )\n",
    "\n",
    "        if len(predict_crops) >= maximum:\n",
    "            break\n",
    "\n",
    "    np.save(f\"tmp/{name}_areas_prediction_{SUBDIVISION_SIZE}.npy\", np.array(predict_crops))\n",
    "    np.save(f\"tmp/{name}_areas_pp_{SUBDIVISION_SIZE}.npy\", np.array(pp_crops))\n",
    "    np.save(f\"tmp/{name}_areas_contact_{SUBDIVISION_SIZE}.npy\", np.array(contact_crops))\n",
    "\n",
    "\n",
    "def format_refiner_data() -> tuple:\n",
    "    train_set = {\n",
    "        \"input\": (\n",
    "            np.load(f\"tmp/train_areas_prediction_{SUBDIVISION_SIZE}.npy\"),\n",
    "            np.load(f\"tmp/train_areas_pp_{SUBDIVISION_SIZE}.npy\")\n",
    "        ),\n",
    "        \"output\": np.load(f\"tmp/train_areas_contact_{SUBDIVISION_SIZE}.npy\"),\n",
    "        \"names\": []\n",
    "    }\n",
    "    validation_set = {\n",
    "        \"input\": (\n",
    "            np.load(f\"tmp/validation_areas_prediction_{SUBDIVISION_SIZE}.npy\"),\n",
    "            np.load(f\"tmp/validation_areas_pp_{SUBDIVISION_SIZE}.npy\")\n",
    "        ),\n",
    "        \"output\": np.load(f\"tmp/validation_areas_contact_{SUBDIVISION_SIZE}.npy\"),\n",
    "        \"names\": []\n",
    "    }\n",
    "    test_set = {\n",
    "        \"input\": (\n",
    "            np.load(f\"tmp/test_areas_prediction_{SUBDIVISION_SIZE}.npy\"),\n",
    "            np.load(f\"tmp/test_areas_pp_{SUBDIVISION_SIZE}.npy\")\n",
    "        ),\n",
    "        \"output\": np.load(f\"tmp/test_areas_contact_{SUBDIVISION_SIZE}.npy\"),\n",
    "        \"names\": []\n",
    "    }\n",
    "    return train_set, validation_set, test_set\n",
    "\n",
    "\n",
    "def refine(estimation: np.ndarray, potential_pairings: np.ndarray, model) -> np.ndarray:\n",
    "    result = np.zeros((SIZE, SIZE))\n",
    "    for v_stride in range(int(SIZE / SUBDIVISION_SIZE)):\n",
    "        row = v_stride * SUBDIVISION_SIZE\n",
    "        for h_stride in range(int(SIZE / SUBDIVISION_SIZE)):\n",
    "            column = h_stride * SUBDIVISION_SIZE\n",
    "            sub_estimation = estimation[\n",
    "                row:row + SUBDIVISION_SIZE, column:column + SUBDIVISION_SIZE\n",
    "            ]\n",
    "            sub_input = potential_pairings[\n",
    "                row:row + SUBDIVISION_SIZE, column:column + SUBDIVISION_SIZE\n",
    "            ]\n",
    "            prediction = model.predict([sub_estimation, sub_input])\n",
    "            result[row:row + SUBDIVISION_SIZE, column:column + SUBDIVISION_SIZE] = prediction\n",
    "    return result\n",
    "\n",
    "\n",
    "def dense_refine(estimation: np.ndarray, potential_pairings: np.ndarray, model) -> np.ndarray:\n",
    "    result = np.zeros((SIZE, SIZE))\n",
    "    for v_stride in range(int(SIZE / SUBDIVISION_SIZE) * 2 - 1):\n",
    "        row = int(v_stride / 2 * SUBDIVISION_SIZE)\n",
    "        for h_stride in range(int(SIZE / SUBDIVISION_SIZE) * 2 - 1):\n",
    "            column = int(h_stride /2 * SUBDIVISION_SIZE)\n",
    "            sub_estimation = estimation[\n",
    "                row:row + SUBDIVISION_SIZE, column:column + SUBDIVISION_SIZE\n",
    "            ]\n",
    "            sub_input = potential_pairings[\n",
    "                row:row + SUBDIVISION_SIZE, column:column + SUBDIVISION_SIZE\n",
    "            ]\n",
    "            prediction = model.predict([sub_estimation, sub_input])\n",
    "            result[row:row + SUBDIVISION_SIZE, column:column + SUBDIVISION_SIZE] += prediction\n",
    "    return result / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MU(matrix: np.ndarray) -> np.ndarray:\n",
    "    return transform.to_monomial_matrix(matrix)\n",
    "\n",
    "\n",
    "def BU(matrix: np.ndarray) -> np.ndarray:\n",
    "    return transform.to_binary_matrix(matrix)\n",
    "\n",
    "\n",
    "def SU(matrix: np.ndarray) -> np.ndarray:\n",
    "    return matrix * matrix.T\n",
    "\n",
    "\n",
    "def CU(matrix: np.ndarray, mask: np.ndarray) -> np.ndarray:\n",
    "    return matrix * mask\n",
    "\n",
    "\n",
    "def TU(matrix: np.ndarray, threshold: float) -> np.ndarray:\n",
    "    return (matrix < threshold) * matrix\n",
    "\n",
    "\n",
    "def NRU(matrix: np.ndarray, threshold: float) -> np.ndarray:\n",
    "    return segment.convolutional_denoise(matrix, threshold=threshold)\n",
    "\n",
    "\n",
    "def NU(matrix: np.ndarray) -> np.ndarray:\n",
    "    normalized = matrix.copy()\n",
    "    maximum = normalized.max()\n",
    "    normalized[normalized == 0] = 2\n",
    "    minimum = normalized.min()\n",
    "    normalized -= minimum\n",
    "    normalized *= 1 / (maximum - minimum)\n",
    "    normalized[normalized > 1] = 0\n",
    "    return normalized\n",
    "\n",
    "\n",
    "def OU(matrix: np.ndarray, mask: np.ndarray) -> np.ndarray:\n",
    "    matrix = CU(matrix, mask)\n",
    "    matrix = SU(matrix)\n",
    "    matrix = MU(matrix)\n",
    "    matrix = BU(matrix)\n",
    "    matrix = NRU(matrix, 2)\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect(model, data: dict, i: int) -> None:\n",
    "    t = data[\"output\"][i]\n",
    "    p = model(data[\"input\"][0][i], data[\"mask\"][i])\n",
    "    visualize.compare_pairings(t, p)\n",
    "    print(f\"F1 score: {evaluate.ContactMatrix.f1(t, p)}\")\n",
    "\n",
    "\n",
    "def measure_performances(model, data) -> list[float]:\n",
    "    scores = []\n",
    "    for i in range(len(data[\"output\"])):\n",
    "        t = data[\"output\"][i]\n",
    "        p = model(data[\"input\"][0][i], data[\"mask\"][i])\n",
    "        f = evaluate.ContactMatrix.f1(t, p)\n",
    "        scores.append(f)\n",
    "    print(f\"F1 scores: {scores}\")\n",
    "    print(f\"Arithmetic mean: {np.mean(scores)}\")\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: Global Estimation (GSE -> OU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-11T21:26:26.051736 > INFO Training the model with 1860 data points.\n",
      "2024-06-11T21:26:26.053740 >     Using 232 data points for validation.\n",
      "2024-06-11T21:26:26.054739 >     Beginning the training.\n",
      "2024-06-11T21:27:04.570590 >   0 / 100 [                                                  ] Loss: 2.10887  Patience: 10\n",
      "2024-06-11T21:27:05.652495 >   1 / 100 [                                                  ] Loss: 2.07830  Patience: 10\n",
      "2024-06-11T21:27:06.687761 >   2 / 100 [=                                                 ] Loss: 2.06632  Patience: 10\n",
      "2024-06-11T21:27:07.597550 >   3 / 100 [=                                                 ] Loss: 2.06026  Patience: 10\n",
      "2024-06-11T21:27:08.564924 >   4 / 100 [==                                                ] Loss: 2.05666  Patience: 10\n",
      "2024-06-11T21:27:09.563894 >   5 / 100 [==                                                ] Loss: 2.05314  Patience: 10\n",
      "2024-06-11T21:27:10.564001 >   6 / 100 [===                                               ] Loss: 2.04998  Patience: 10\n",
      "2024-06-11T21:27:11.557968 >   7 / 100 [===                                               ] Loss: 2.04747  Patience: 10\n",
      "2024-06-11T21:27:12.496419 >   8 / 100 [====                                              ] Loss: 2.04607  Patience: 10\n",
      "2024-06-11T21:27:13.425636 >   9 / 100 [====                                              ] Loss: 2.04613  Patience: 9\n",
      "2024-06-11T21:27:14.382899 >  10 / 100 [=====                                             ] Loss: 2.04498  Patience: 9\n",
      "2024-06-11T21:27:15.332437 >  11 / 100 [=====                                             ] Loss: 2.04295  Patience: 9\n",
      "2024-06-11T21:27:16.289202 >  12 / 100 [======                                            ] Loss: 2.04141  Patience: 9\n",
      "2024-06-11T21:27:17.227495 >  13 / 100 [======                                            ] Loss: 2.03996  Patience: 9\n",
      "2024-06-11T21:27:18.259011 >  14 / 100 [=======                                           ] Loss: 2.03821  Patience: 9\n",
      "2024-06-11T21:27:19.236693 >  15 / 100 [=======                                           ] Loss: 2.03648  Patience: 9\n",
      "2024-06-11T21:27:20.181973 >  16 / 100 [========                                          ] Loss: 2.03541  Patience: 9\n",
      "2024-06-11T21:27:21.139332 >  17 / 100 [========                                          ] Loss: 2.03469  Patience: 9\n",
      "2024-06-11T21:27:22.077932 >  18 / 100 [=========                                         ] Loss: 2.03455  Patience: 9\n",
      "2024-06-11T21:27:23.052088 >  19 / 100 [=========                                         ] Loss: 2.03410  Patience: 9\n",
      "2024-06-11T21:27:23.982352 >  20 / 100 [==========                                        ] Loss: 2.03322  Patience: 9\n",
      "2024-06-11T21:27:24.925024 >  21 / 100 [==========                                        ] Loss: 2.03221  Patience: 9\n",
      "2024-06-11T21:27:25.884193 >  22 / 100 [===========                                       ] Loss: 2.03150  Patience: 9\n",
      "2024-06-11T21:27:26.829179 >  23 / 100 [===========                                       ] Loss: 2.03135  Patience: 9\n",
      "2024-06-11T21:27:27.782423 >  24 / 100 [============                                      ] Loss: 2.03054  Patience: 9\n",
      "2024-06-11T21:27:28.727018 >  25 / 100 [============                                      ] Loss: 2.02972  Patience: 9\n",
      "2024-06-11T21:27:29.654339 >  26 / 100 [=============                                     ] Loss: 2.02957  Patience: 9\n",
      "2024-06-11T21:27:30.582096 >  27 / 100 [=============                                     ] Loss: 2.02964  Patience: 8\n",
      "2024-06-11T21:27:31.509499 >  28 / 100 [==============                                    ] Loss: 2.02935  Patience: 8\n",
      "2024-06-11T21:27:32.454887 >  29 / 100 [==============                                    ] Loss: 2.02853  Patience: 8\n",
      "2024-06-11T21:27:33.420124 >  30 / 100 [===============                                   ] Loss: 2.02964  Patience: 7\n",
      "2024-06-11T21:27:34.370683 >  31 / 100 [===============                                   ] Loss: 2.02912  Patience: 6\n",
      "2024-06-11T21:27:35.311088 >  32 / 100 [================                                  ] Loss: 2.02808  Patience: 6\n",
      "2024-06-11T21:27:36.262106 >  33 / 100 [================                                  ] Loss: 2.02739  Patience: 6\n",
      "2024-06-11T21:27:37.228214 >  34 / 100 [=================                                 ] Loss: 2.02750  Patience: 5\n",
      "2024-06-11T21:27:38.210515 >  35 / 100 [=================                                 ] Loss: 2.02703  Patience: 5\n",
      "2024-06-11T21:27:39.180957 >  36 / 100 [==================                                ] Loss: 2.02613  Patience: 5\n",
      "2024-06-11T21:27:40.200232 >  37 / 100 [==================                                ] Loss: 2.02614  Patience: 4\n",
      "2024-06-11T21:27:41.170412 >  38 / 100 [===================                               ] Loss: 2.02569  Patience: 4\n",
      "2024-06-11T21:27:42.153000 >  39 / 100 [===================                               ] Loss: 2.02702  Patience: 3\n",
      "2024-06-11T21:27:43.077140 >  40 / 100 [====================                              ] Loss: 2.02708  Patience: 2\n",
      "2024-06-11T21:27:44.013383 >  41 / 100 [====================                              ] Loss: 2.02510  Patience: 2\n",
      "2024-06-11T21:27:44.971773 >  42 / 100 [=====================                             ] Loss: 2.02503  Patience: 2\n",
      "2024-06-11T21:27:45.905560 >  43 / 100 [=====================                             ] Loss: 2.02424  Patience: 2\n",
      "2024-06-11T21:27:46.852000 >  44 / 100 [======================                            ] Loss: 2.02462  Patience: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data, validation_data, test_data = format_data()\n",
    "\n",
    "N_MAX_EPOCHS = 100\n",
    "GSE = deep.NN(\n",
    "    model=cnn.UNet2D(SIZE, 4),\n",
    "    n_epochs=N_MAX_EPOCHS,\n",
    "    optimizer=optim.Adam,\n",
    "    loss_fn=nn.CrossEntropyLoss,\n",
    "    use_half=False,\n",
    "    patience=10,\n",
    "    verbosity=2,\n",
    ")\n",
    "GSE.train(train_data, validation_data)\n",
    "\n",
    "def model_1(pp: np.ndarray, mask: np.ndarray):\n",
    "    x = GSE.predict([pp])\n",
    "    x = NU(x)\n",
    "    x = OU(x, mask)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1_results = measure_performances(model_1, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2: Refined Predictions (GSE -> LSR -> OU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1860/1860 [00:04<00:00, 417.75it/s]\n",
      "100%|██████████| 232/232 [00:00<00:00, 419.79it/s]\n",
      "100%|██████████| 232/232 [00:00<00:00, 495.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample subregions from the predictions and input data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1860/1860 [00:00<00:00, 3391.33it/s]\n",
      "100%|██████████| 232/232 [00:00<00:00, 3532.35it/s]\n",
      "100%|██████████| 232/232 [00:00<00:00, 3274.87it/s]\n"
     ]
    }
   ],
   "source": [
    "save_global_estimations(GSE, train_data, \"train_predictions.npy\")\n",
    "save_global_estimations(GSE, validation_data, \"validation_predictions.npy\")\n",
    "save_global_estimations(GSE, test_data, \"test_predictions.npy\")\n",
    "\n",
    "print(\"Sample subregions from the predictions and input data.\")\n",
    "sample_crops(np.load(\"tmp/train_predictions.npy\"), train_data, \"train\", 100_000)\n",
    "sample_crops(np.load(\"tmp/validation_predictions.npy\"), validation_data, \"validation\", 25_000)\n",
    "sample_crops(np.load(\"tmp/test_predictions.npy\"), test_data, \"test\", 25_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-11T09:46:49.357765 > INFO Training the model with 44046 data points.\n",
      "2024-06-11T09:46:49.358790 >     Using 5494 data points for validation.\n",
      "2024-06-11T09:46:49.358790 >     Beginning the training.\n",
      "2024-06-11T09:46:51.995363 >   0 / 500 [                                                  ] Loss: 0.64707  Patience: 10\n",
      "2024-06-11T09:46:53.832983 >   1 / 500 [                                                  ] Loss: 0.64123  Patience: 10\n",
      "2024-06-11T09:46:55.565195 >   2 / 500 [                                                  ] Loss: 0.63286  Patience: 10\n",
      "2024-06-11T09:46:57.346440 >   3 / 500 [                                                  ] Loss: 0.62898  Patience: 10\n",
      "2024-06-11T09:46:59.088846 >   4 / 500 [                                                  ] Loss: 0.62746  Patience: 10\n",
      "2024-06-11T09:47:00.883817 >   5 / 500 [                                                  ] Loss: 0.62638  Patience: 10\n",
      "2024-06-11T09:47:02.653033 >   6 / 500 [                                                  ] Loss: 0.62458  Patience: 10\n",
      "2024-06-11T09:47:04.410172 >   7 / 500 [                                                  ] Loss: 0.62458  Patience: 10\n",
      "2024-06-11T09:47:06.218451 >   8 / 500 [                                                  ] Loss: 0.62417  Patience: 10\n",
      "2024-06-11T09:47:08.130665 >   9 / 500 [                                                  ] Loss: 0.62431  Patience: 9\n",
      "2024-06-11T09:47:09.909409 >  10 / 500 [=                                                 ] Loss: 0.62341  Patience: 9\n",
      "2024-06-11T09:47:11.717586 >  11 / 500 [=                                                 ] Loss: 0.62362  Patience: 8\n",
      "2024-06-11T09:47:13.571851 >  12 / 500 [=                                                 ] Loss: 0.62396  Patience: 7\n",
      "2024-06-11T09:47:15.360442 >  13 / 500 [=                                                 ] Loss: 0.62342  Patience: 6\n",
      "2024-06-11T09:47:17.165842 >  14 / 500 [=                                                 ] Loss: 0.62321  Patience: 6\n",
      "2024-06-11T09:47:18.932727 >  15 / 500 [=                                                 ] Loss: 0.62303  Patience: 6\n",
      "2024-06-11T09:47:20.763856 >  16 / 500 [=                                                 ] Loss: 0.62328  Patience: 5\n",
      "2024-06-11T09:47:22.548568 >  17 / 500 [=                                                 ] Loss: 0.62375  Patience: 4\n",
      "2024-06-11T09:47:24.317976 >  18 / 500 [=                                                 ] Loss: 0.62268  Patience: 4\n",
      "2024-06-11T09:47:26.101370 >  19 / 500 [=                                                 ] Loss: 0.62328  Patience: 3\n",
      "2024-06-11T09:47:27.903280 >  20 / 500 [==                                                ] Loss: 0.62314  Patience: 2\n",
      "2024-06-11T09:47:29.679948 >  21 / 500 [==                                                ] Loss: 0.62264  Patience: 2\n",
      "2024-06-11T09:47:31.466160 >  22 / 500 [==                                                ] Loss: 0.62232  Patience: 2\n",
      "2024-06-11T09:47:33.382865 >  23 / 500 [==                                                ] Loss: 0.62225  Patience: 2\n",
      "2024-06-11T09:47:35.093483 >  24 / 500 [==                                                ] Loss: 0.62210  Patience: 2\n",
      "2024-06-11T09:47:36.915868 >  25 / 500 [==                                                ] Loss: 0.62159  Patience: 2\n",
      "2024-06-11T09:47:38.738790 >  26 / 500 [==                                                ] Loss: 0.62184  Patience: 1\n",
      "2024-06-11T09:47:40.574843 >  27 / 500 [==                                                ] Loss: 0.62147  Patience: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "refiner_train_data, refiner_validation_data, refiner_test_data = format_refiner_data()\n",
    "\n",
    "N_MAX_EPOCHS = 500\n",
    "LSR = deep.NN(\n",
    "    model=cnn.UNet2DRefiner(SUBDIVISION_SIZE, 3),\n",
    "    n_epochs=N_MAX_EPOCHS,\n",
    "    optimizer=optim.Adam,\n",
    "    loss_fn=nn.CrossEntropyLoss,\n",
    "    loss_fn_args=None,\n",
    "    use_half=False,\n",
    "    patience=10,\n",
    "    verbosity=2,\n",
    "    batch=256\n",
    ")\n",
    "LSR.train(refiner_train_data, refiner_validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 scores: [0.11764705926179886, 0.49122804403305054, 0.5084745287895203, 0.3333333432674408, 0.0, 0.11764706671237946, 0.0714285746216774, 0.2857142984867096, 0.7878788113594055, 0.20338982343673706, 0.25806450843811035, 0.06666667014360428, 0.555555522441864, 0.3137255012989044, 0.3606557548046112, 0.06060606241226196, 0.5, 0.2142857164144516, 0.25, 0.4444444477558136, 0.07407407462596893, 0.14814814925193787, 0.1666666716337204, 0.40816327929496765, 0.25, 0.4000000059604645, 0.25, 0.4642857313156128, 0.4761904776096344, 0.2641509473323822, 0.052631575614213943, 0.6000000238418579, 0.0, 0.42307692766189575, 0.42105263471603394, 0.0, 0.4878048598766327, 0.19607844948768616, 0.5454545021057129, 0.2380952388048172, 0.6666666865348816, 0.1860465109348297, 0.32786887884140015, 0.042553193867206573, 0.5789474248886108, 0.5079365372657776, 0.3137255012989044, 0.06451612710952759, 0.2857142686843872, 0.25, 0.4313725531101227, 0.0, 0.5396825075149536, 0.3333333432674408, 0.25, 0.610169529914856, 0.4000000059604645, 0.1568627506494522, 0.5753424763679504, 0.26923075318336487, 0.5283018946647644, 0.4615384638309479, 0.4848484992980957, 0.5245901942253113, 0.4680851101875305, 0.2068965584039688, 0.4333333373069763, 0.4000000059604645, 0.3030302822589874, 0.2666666805744171, 0.42307692766189575, 0.3606557548046112, 0.1702127754688263, 0.47999998927116394, 0.4000000059604645, 0.5714285373687744, 0.40000003576278687, 0.1764705926179886, 0.0, 0.5789474248886108, 0.48275861144065857, 0.0, 0.0, 0.21276594698429108, 0.3333333730697632, 0.3414634168148041, 0.1875, 0.6229508519172668, 0.1818181872367859, 0.162162184715271, 0.0, 0.260869562625885, 0.5000000596046448, 0.0, 0.05000000074505806, 0.12121212482452393, 0.0, 0.31111109256744385, 0.517241358757019, 0.07692307978868484, 0.3934426009654999, 0.3272727429866791, 0.13333334028720856, 0.0, 0.1818181872367859, 0.3934426009654999, 0.5901639461517334, 0.3214285671710968, 0.4406780004501343, 0.10256410390138626, 0.1304347962141037, 0.2800000309944153, 0.375, 0.18867924809455872, 0.3928571343421936, 0.6349206566810608, 0.17777776718139648, 0.4761904776096344, 0.12121212482452393, 0.45614033937454224, 0.09756097942590714, 0.4838709831237793, 0.4583333134651184, 0.3606557548046112, 0.3103448450565338, 0.4000000059604645, 0.40625, 0.4000000059604645, 0.5084745287895203, 0.375, 0.5084746479988098, 0.26229509711265564, 0.06896551698446274, 0.3692307770252228, 0.1568627506494522, 0.42307692766189575, 0.09090909361839294, 0.17391304671764374, 0.42553192377090454, 0.1568627506494522, 0.14814816415309906, 0.0, 0.5714285373687744, 0.0, 0.4516128897666931, 0.3829787075519562, 0.12903225421905518, 0.1250000149011612, 0.14814814925193787, 0.30188679695129395, 0.6521738767623901, 0.5714285969734192, 0.21276596188545227, 0.260869562625885, 0.0, 0.4150943458080292, 0.4000000059604645, 0.6521738767623901, 0.42553189396858215, 0.320000022649765, 0.3050847351551056, 0.23529411852359772, 0.5405405759811401, 0.2790697515010834, 0.5714285373687744, 0.0, 0.34285712242126465, 0.6060606241226196, 0.4561403691768646, 0.3103448450565338, 0.1090909093618393, 0.260869562625885, 0.5789474248886108, 0.4150943458080292, 0.1600000113248825, 0.3137255012989044, 0.0, 0.5483871102333069, 0.30434781312942505, 0.30188679695129395, 0.25, 0.08888889104127884, 0.4444444477558136, 0.1538461595773697, 0.17142857611179352, 0.30188682675361633, 0.13793103396892548, 0.5573770999908447, 0.12903225421905518, 0.46666666865348816, 0.3571428954601288, 0.21621622145175934, 0.18518517911434174, 0.07407407462596893, 0.19354838132858276, 0.20000001788139343, 0.4313725531101227, 0.1621621549129486, 0.4848484992980957, 0.24242423474788666, 0.08695652335882187, 0.3125, 0.1818181872367859, 0.1764705777168274, 0.4528302252292633, 0.324324369430542, 0.22727273404598236, 0.3829787075519562, 0.4285714328289032, 0.0, 0.1599999964237213, 0.21276594698429108, 0.43478262424468994, 0.2352941334247589, 0.2222222238779068, 0.32786884903907776, 0.2800000011920929, 0.4897959232330322, 0.41379314661026, 0.4313725531101227, 0.3571428656578064, 0.4848484694957733, 0.3333333432674408, 0.3571428656578064, 0.20000001788139343, 0.5757575631141663, 0.4516128897666931, 0.7213115096092224, 0.27586203813552856, 0.260869562625885, 0.31111112236976624, 0.5185185074806213]\n",
      "Arithmetic mean: 0.309637067935847\n"
     ]
    }
   ],
   "source": [
    "def model_2(pp: np.ndarray, mask: np.ndarray):\n",
    "    x = GSE.predict([pp])\n",
    "    #x += refine(x, pp, LSR)\n",
    "    x *= dense_refine(x, pp, LSR)\n",
    "    x = OU(x, mask)\n",
    "    return x\n",
    "# 0.231\n",
    "\n",
    "model_2_results = measure_performances(model_2, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3: Refined Predictions (GSE -> [CU -> NRU] -> LSR -> OU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1860/1860 [00:04<00:00, 384.98it/s]\n",
      "100%|██████████| 232/232 [00:00<00:00, 360.20it/s]\n",
      "100%|██████████| 232/232 [00:00<00:00, 396.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample subregions from the predictions and input data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1860/1860 [00:00<00:00, 2960.12it/s]\n",
      "100%|██████████| 232/232 [00:00<00:00, 3223.99it/s]\n",
      "100%|██████████| 232/232 [00:00<00:00, 3263.87it/s]\n"
     ]
    }
   ],
   "source": [
    "class Estimator:\n",
    "    def predict(pp) -> np.ndarray:\n",
    "        estimation = GSE.predict(pp)\n",
    "        mask = pp[0].copy()\n",
    "        mask[mask > 0] = 1\n",
    "        estimation *= mask\n",
    "        estimation = SU(estimation)\n",
    "        return estimation#NRU(estimation, 2)\n",
    "\n",
    "\n",
    "save_global_estimations(Estimator, train_data, \"train_predictions.npy\")\n",
    "save_global_estimations(Estimator, validation_data, \"validation_predictions.npy\")\n",
    "save_global_estimations(Estimator, test_data, \"test_predictions.npy\")\n",
    "\n",
    "print(\"Sample subregions from the predictions and input data.\")\n",
    "sample_crops(np.load(\"tmp/train_predictions.npy\"), train_data, \"train\", 100_000)\n",
    "sample_crops(np.load(\"tmp/validation_predictions.npy\"), validation_data, \"validation\", 25_000)\n",
    "sample_crops(np.load(\"tmp/test_predictions.npy\"), test_data, \"test\", 25_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-11T09:50:15.848119 > INFO Training the model with 44046 data points.\n",
      "2024-06-11T09:50:15.849529 >     Using 5494 data points for validation.\n",
      "2024-06-11T09:50:15.850576 >     Beginning the training.\n",
      "2024-06-11T09:50:18.663487 >   0 / 500 [                                                  ] Loss: 0.70687  Patience: 5\n",
      "2024-06-11T09:50:20.618088 >   1 / 500 [                                                  ] Loss: 0.70214  Patience: 5\n",
      "2024-06-11T09:50:22.356590 >   2 / 500 [                                                  ] Loss: 0.69874  Patience: 5\n",
      "2024-06-11T09:50:24.136345 >   3 / 500 [                                                  ] Loss: 0.68917  Patience: 5\n",
      "2024-06-11T09:50:25.798763 >   4 / 500 [                                                  ] Loss: 0.68319  Patience: 5\n",
      "2024-06-11T09:50:27.583848 >   5 / 500 [                                                  ] Loss: 0.68189  Patience: 5\n",
      "2024-06-11T09:50:29.356517 >   6 / 500 [                                                  ] Loss: 0.68110  Patience: 5\n",
      "2024-06-11T09:50:31.434212 >   7 / 500 [                                                  ] Loss: 0.68040  Patience: 5\n",
      "2024-06-11T09:50:33.133058 >   8 / 500 [                                                  ] Loss: 0.67974  Patience: 5\n",
      "2024-06-11T09:50:35.010450 >   9 / 500 [                                                  ] Loss: 0.67932  Patience: 5\n",
      "2024-06-11T09:50:36.846093 >  10 / 500 [=                                                 ] Loss: 0.67902  Patience: 5\n",
      "2024-06-11T09:50:38.588807 >  11 / 500 [=                                                 ] Loss: 0.67881  Patience: 5\n",
      "2024-06-11T09:50:40.357103 >  12 / 500 [=                                                 ] Loss: 0.67855  Patience: 5\n",
      "2024-06-11T09:50:42.186465 >  13 / 500 [=                                                 ] Loss: 0.67830  Patience: 5\n",
      "2024-06-11T09:50:44.131907 >  14 / 500 [=                                                 ] Loss: 0.67803  Patience: 5\n",
      "2024-06-11T09:50:45.905192 >  15 / 500 [=                                                 ] Loss: 0.67778  Patience: 5\n",
      "2024-06-11T09:50:47.714945 >  16 / 500 [=                                                 ] Loss: 0.67761  Patience: 5\n",
      "2024-06-11T09:50:49.493854 >  17 / 500 [=                                                 ] Loss: 0.67729  Patience: 5\n",
      "2024-06-11T09:50:51.423809 >  18 / 500 [=                                                 ] Loss: 0.67699  Patience: 5\n",
      "2024-06-11T09:50:53.127946 >  19 / 500 [=                                                 ] Loss: 0.67675  Patience: 5\n",
      "2024-06-11T09:50:54.868762 >  20 / 500 [==                                                ] Loss: 0.67651  Patience: 5\n",
      "2024-06-11T09:50:56.620748 >  21 / 500 [==                                                ] Loss: 0.67623  Patience: 5\n",
      "2024-06-11T09:50:58.409900 >  22 / 500 [==                                                ] Loss: 0.67599  Patience: 5\n",
      "2024-06-11T09:51:00.163222 >  23 / 500 [==                                                ] Loss: 0.67573  Patience: 5\n",
      "2024-06-11T09:51:01.821905 >  24 / 500 [==                                                ] Loss: 0.67551  Patience: 5\n",
      "2024-06-11T09:51:03.547981 >  25 / 500 [==                                                ] Loss: 0.67538  Patience: 5\n",
      "2024-06-11T09:51:05.298886 >  26 / 500 [==                                                ] Loss: 0.67520  Patience: 5\n",
      "2024-06-11T09:51:07.023511 >  27 / 500 [==                                                ] Loss: 0.67500  Patience: 5\n",
      "2024-06-11T09:51:08.824634 >  28 / 500 [==                                                ] Loss: 0.67488  Patience: 5\n",
      "2024-06-11T09:51:10.597178 >  29 / 500 [==                                                ] Loss: 0.67467  Patience: 5\n",
      "2024-06-11T09:51:12.281235 >  30 / 500 [===                                               ] Loss: 0.67456  Patience: 5\n",
      "2024-06-11T09:51:14.006527 >  31 / 500 [===                                               ] Loss: 0.67436  Patience: 5\n",
      "2024-06-11T09:51:15.743540 >  32 / 500 [===                                               ] Loss: 0.67416  Patience: 5\n",
      "2024-06-11T09:51:17.623654 >  33 / 500 [===                                               ] Loss: 0.67407  Patience: 5\n",
      "2024-06-11T09:51:19.351681 >  34 / 500 [===                                               ] Loss: 0.67390  Patience: 5\n",
      "2024-06-11T09:51:21.141350 >  35 / 500 [===                                               ] Loss: 0.67376  Patience: 5\n",
      "2024-06-11T09:51:22.789738 >  36 / 500 [===                                               ] Loss: 0.67358  Patience: 5\n",
      "2024-06-11T09:51:24.577037 >  37 / 500 [===                                               ] Loss: 0.67346  Patience: 5\n",
      "2024-06-11T09:51:26.349678 >  38 / 500 [===                                               ] Loss: 0.67333  Patience: 5\n",
      "2024-06-11T09:51:28.153489 >  39 / 500 [===                                               ] Loss: 0.67318  Patience: 5\n",
      "2024-06-11T09:51:29.943943 >  40 / 500 [====                                              ] Loss: 0.67315  Patience: 5\n",
      "2024-06-11T09:51:31.688618 >  41 / 500 [====                                              ] Loss: 0.67299  Patience: 5\n",
      "2024-06-11T09:51:33.368548 >  42 / 500 [====                                              ] Loss: 0.67284  Patience: 5\n",
      "2024-06-11T09:51:35.069592 >  43 / 500 [====                                              ] Loss: 0.67270  Patience: 5\n",
      "2024-06-11T09:51:36.749775 >  44 / 500 [====                                              ] Loss: 0.67256  Patience: 5\n",
      "2024-06-11T09:51:38.451452 >  45 / 500 [====                                              ] Loss: 0.67248  Patience: 5\n",
      "2024-06-11T09:51:40.323632 >  46 / 500 [====                                              ] Loss: 0.67237  Patience: 5\n",
      "2024-06-11T09:51:42.078792 >  47 / 500 [====                                              ] Loss: 0.67233  Patience: 5\n",
      "2024-06-11T09:51:43.825463 >  48 / 500 [====                                              ] Loss: 0.67222  Patience: 5\n",
      "2024-06-11T09:51:45.592864 >  49 / 500 [====                                              ] Loss: 0.67206  Patience: 5\n",
      "2024-06-11T09:51:47.343802 >  50 / 500 [=====                                             ] Loss: 0.67206  Patience: 5\n",
      "2024-06-11T09:51:49.099631 >  51 / 500 [=====                                             ] Loss: 0.67186  Patience: 5\n",
      "2024-06-11T09:51:50.861030 >  52 / 500 [=====                                             ] Loss: 0.67183  Patience: 5\n",
      "2024-06-11T09:51:52.627983 >  53 / 500 [=====                                             ] Loss: 0.67176  Patience: 5\n",
      "2024-06-11T09:51:54.372061 >  54 / 500 [=====                                             ] Loss: 0.67158  Patience: 5\n",
      "2024-06-11T09:51:56.095788 >  55 / 500 [=====                                             ] Loss: 0.67152  Patience: 5\n",
      "2024-06-11T09:51:57.824853 >  56 / 500 [=====                                             ] Loss: 0.67152  Patience: 4\n",
      "2024-06-11T09:51:59.558044 >  57 / 500 [=====                                             ] Loss: 0.67138  Patience: 4\n",
      "2024-06-11T09:52:01.284324 >  58 / 500 [=====                                             ] Loss: 0.67140  Patience: 3\n",
      "2024-06-11T09:52:03.084297 >  59 / 500 [=====                                             ] Loss: 0.67124  Patience: 3\n",
      "2024-06-11T09:52:04.853339 >  60 / 500 [======                                            ] Loss: 0.67115  Patience: 3\n",
      "2024-06-11T09:52:06.616984 >  61 / 500 [======                                            ] Loss: 0.67113  Patience: 3\n",
      "2024-06-11T09:52:08.349717 >  62 / 500 [======                                            ] Loss: 0.67100  Patience: 3\n",
      "2024-06-11T09:52:10.126547 >  63 / 500 [======                                            ] Loss: 0.67085  Patience: 3\n",
      "2024-06-11T09:52:11.931658 >  64 / 500 [======                                            ] Loss: 0.67079  Patience: 3\n",
      "2024-06-11T09:52:13.674407 >  65 / 500 [======                                            ] Loss: 0.67083  Patience: 2\n",
      "2024-06-11T09:52:15.493911 >  66 / 500 [======                                            ] Loss: 0.67072  Patience: 2\n",
      "2024-06-11T09:52:17.231334 >  67 / 500 [======                                            ] Loss: 0.67056  Patience: 2\n",
      "2024-06-11T09:52:18.974899 >  68 / 500 [======                                            ] Loss: 0.67048  Patience: 2\n",
      "2024-06-11T09:52:20.678631 >  69 / 500 [======                                            ] Loss: 0.67033  Patience: 2\n",
      "2024-06-11T09:52:22.446428 >  70 / 500 [=======                                           ] Loss: 0.67040  Patience: 1\n",
      "2024-06-11T09:52:24.259649 >  71 / 500 [=======                                           ] Loss: 0.67024  Patience: 1\n",
      "2024-06-11T09:52:26.031428 >  72 / 500 [=======                                           ] Loss: 0.67024  Patience: 1\n",
      "2024-06-11T09:52:27.816287 >  73 / 500 [=======                                           ] Loss: 0.67023  Patience: 1\n",
      "2024-06-11T09:52:29.608860 >  74 / 500 [=======                                           ] Loss: 0.67009  Patience: 1\n",
      "2024-06-11T09:52:31.326812 >  75 / 500 [=======                                           ] Loss: 0.67000  Patience: 1\n",
      "2024-06-11T09:52:33.050714 >  76 / 500 [=======                                           ] Loss: 0.66992  Patience: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "refiner_train_data, refiner_validation_data, refiner_test_data = format_refiner_data()\n",
    "\n",
    "N_MAX_EPOCHS = 500\n",
    "LSR = deep.NN(\n",
    "    model=cnn.UNet2DRefiner(SUBDIVISION_SIZE, 3),\n",
    "    n_epochs=N_MAX_EPOCHS,\n",
    "    optimizer=optim.Adam,\n",
    "    loss_fn=nn.CrossEntropyLoss,\n",
    "    loss_fn_args=None,\n",
    "    use_half=False,\n",
    "    patience=5,\n",
    "    verbosity=2,\n",
    "    batch=256\n",
    ")\n",
    "LSR.train(refiner_train_data, refiner_validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 scores: [0.1818181872367859, 0.5079364776611328, 0.625, 0.6557376980781555, 0.0, 0.14814814925193787, 0.24242424964904785, 0.2222222238779068, 0.7058823704719543, 0.6774193644523621, 0.27586206793785095, 0.0625, 0.523809552192688, 0.4590164124965668, 0.6666666865348816, 0.1818181723356247, 0.5806452035903931, 0.5588235855102539, 0.4848484992980957, 0.46875, 0.4126984179019928, 0.12121211737394333, 0.277777761220932, 0.5901638865470886, 0.1785714328289032, 0.5970149040222168, 0.636363685131073, 0.53125, 0.34285712242126465, 0.4912280738353729, 0.19230768084526062, 0.5806451439857483, 0.111111119389534, 0.6428571939468384, 0.6969696879386902, 0.0, 0.6000000238418579, 0.4262295365333557, 0.5, 0.1818181872367859, 0.5846154093742371, 0.5333333015441895, 0.6268656849861145, 0.20338982343673706, 0.7222222685813904, 0.7462686896324158, 0.17241379618644714, 0.06060606613755226, 0.23255813121795654, 0.2631579041481018, 0.4642857015132904, 0.0, 0.8235294222831726, 0.5573769807815552, 0.10526315122842789, 0.6315789222717285, 0.5, 0.46666666865348816, 0.6578947305679321, 0.6333333253860474, 0.5806451439857483, 0.5, 0.222222238779068, 0.8333333134651184, 0.3913043737411499, 0.3636363744735718, 0.5396825671195984, 0.7931033968925476, 0.2666666805744171, 0.35294121503829956, 0.46875, 0.6250000596046448, 0.1304347962141037, 0.5517241358757019, 0.48148149251937866, 0.6575342416763306, 0.47999998927116394, 0.0, 0.0, 0.7222222685813904, 0.2068965584039688, 0.033898305147886276, 0.0, 0.1702127605676651, 0.5217391848564148, 0.4864864945411682, 0.5128205418586731, 0.4262295067310333, 0.36734694242477417, 0.27586206793785095, 0.0, 0.5263157486915588, 0.3529411852359772, 0.5, 0.21276594698429108, 0.17647060751914978, 0.0, 0.36000001430511475, 0.36666667461395264, 0.1875, 0.6666666269302368, 0.30000001192092896, 0.20000000298023224, 0.0, 0.3888888657093048, 0.591549277305603, 0.6557376980781555, 0.5245901346206665, 0.7301587462425232, 0.2926829159259796, 0.5, 0.35087716579437256, 0.4590164124965668, 0.3835616409778595, 0.59375, 0.4814814627170563, 0.08888888359069824, 0.5079364776611328, 0.10810811072587967, 0.6000000238418579, 0.09302325546741486, 0.5762711763381958, 0.25, 0.5671641826629639, 0.4482758641242981, 0.5573770999908447, 0.5161290168762207, 0.2962963283061981, 0.6769230961799622, 0.5333333015441895, 0.4761904776096344, 0.5970149040222168, 0.20338982343673706, 0.3636363744735718, 0.2857142686843872, 0.3272727131843567, 0.1568627506494522, 0.23529411852359772, 0.5090909600257874, 0.5862069129943848, 0.40740740299224854, 0.111111119389534, 0.5555555820465088, 0.0, 0.4000000059604645, 0.5517241358757019, 0.42105263471603394, 0.5333333015441895, 0.3076923191547394, 0.6250000596046448, 0.4583333432674408, 0.6578947305679321, 0.0952381044626236, 0.4814814627170563, 0.0, 0.5517241954803467, 0.6060606241226196, 0.18181820213794708, 0.6792452931404114, 0.535714328289032, 0.5454545617103577, 0.42105263471603394, 0.4117647111415863, 0.2499999701976776, 0.6052631139755249, 0.4000000059604645, 0.19354839622974396, 0.6969696879386902, 0.5373134613037109, 0.53125, 0.4074074327945709, 0.37735849618911743, 0.24242424964904785, 0.5, 0.6086955666542053, 0.38596490025520325, 0.0, 0.5079364776611328, 0.42553192377090454, 0.3448275923728943, 0.3499999940395355, 0.24000000953674316, 0.5937499403953552, 0.324324369430542, 0.6111111640930176, 0.42105263471603394, 0.27586206793785095, 0.6666666269302368, 0.11428571492433548, 0.6666666865348816, 0.38596493005752563, 0.3888888955116272, 0.4137931168079376, 0.5217391848564148, 0.2857142984867096, 0.5405405759811401, 0.260869562625885, 0.2777777910232544, 0.222222238779068, 0.29629629850387573, 0.0, 0.17647060751914978, 0.4722222089767456, 0.0, 0.3076923191547394, 0.3589743971824646, 0.3478260934352875, 0.5, 0.596491277217865, 0.0, 0.22727273404598236, 0.6000000238418579, 0.4761905074119568, 0.2857142686843872, 0.5660377144813538, 0.7042254209518433, 0.59375, 0.5373134016990662, 0.3030303120613098, 0.4482758641242981, 0.575757622718811, 0.637681245803833, 0.6060606241226196, 0.5151514410972595, 0.4126984179019928, 0.625, 0.5128205418586731, 0.5964912176132202, 0.5970149636268616, 0.20408163964748383, 0.3461538851261139, 0.3636363744735718]\n",
      "Arithmetic mean: 0.40756324145557554\n"
     ]
    }
   ],
   "source": [
    "def model_3(pp: np.ndarray, mask: np.ndarray):\n",
    "    x = Estimator.predict([pp])\n",
    "    #y = refine(x, pp, LSR)\n",
    "    y = dense_refine(x, pp, LSR)\n",
    "    y *= mask\n",
    "    x += y\n",
    "    x = OU(x, mask)\n",
    "    return x\n",
    "\n",
    "\n",
    "model_3_results = measure_performances(model_3, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 4: Refined Predictions (GSE -> OU -> LSR -> OU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1860/1860 [00:08<00:00, 208.25it/s]\n",
      "100%|██████████| 232/232 [00:01<00:00, 211.37it/s]\n",
      "100%|██████████| 232/232 [00:00<00:00, 238.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample subregions from the predictions and input data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1860/1860 [00:00<00:00, 3026.20it/s]\n",
      "100%|██████████| 232/232 [00:00<00:00, 3575.56it/s]\n",
      "100%|██████████| 232/232 [00:00<00:00, 3438.32it/s]\n"
     ]
    }
   ],
   "source": [
    "class Estimator:\n",
    "    def predict(pp) -> np.ndarray:\n",
    "        estimation = GSE.predict(pp)\n",
    "        mask = pp[0].copy()\n",
    "        mask[mask > 0] = 1\n",
    "        return OU(estimation, mask)\n",
    "\n",
    "\n",
    "save_global_estimations(Estimator, train_data, \"train_predictions.npy\")\n",
    "save_global_estimations(Estimator, validation_data, \"validation_predictions.npy\")\n",
    "save_global_estimations(Estimator, test_data, \"test_predictions.npy\")\n",
    "\n",
    "print(\"Sample subregions from the predictions and input data.\")\n",
    "sample_crops(np.load(\"tmp/train_predictions.npy\"), train_data, \"train\", 100_000)\n",
    "sample_crops(np.load(\"tmp/validation_predictions.npy\"), validation_data, \"validation\", 25_000)\n",
    "sample_crops(np.load(\"tmp/test_predictions.npy\"), test_data, \"test\", 25_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-11T09:41:31.504071 > INFO Training the model with 39311 data points.\n",
      "2024-06-11T09:41:31.505087 >     Using 4962 data points for validation.\n",
      "2024-06-11T09:41:31.505087 >     Beginning the training.\n",
      "2024-06-11T09:41:33.761692 >   0 / 500 [                                                  ] Loss: 0.83845  Patience: 10\n",
      "2024-06-11T09:41:35.384722 >   1 / 500 [                                                  ] Loss: 0.82758  Patience: 10\n",
      "2024-06-11T09:41:36.969953 >   2 / 500 [                                                  ] Loss: 0.82425  Patience: 10\n",
      "2024-06-11T09:41:38.552693 >   3 / 500 [                                                  ] Loss: 0.82206  Patience: 10\n",
      "2024-06-11T09:41:40.266189 >   4 / 500 [                                                  ] Loss: 0.82020  Patience: 10\n",
      "2024-06-11T09:41:42.031845 >   5 / 500 [                                                  ] Loss: 0.81756  Patience: 10\n",
      "2024-06-11T09:41:43.705022 >   6 / 500 [                                                  ] Loss: 0.81209  Patience: 10\n",
      "2024-06-11T09:41:45.345009 >   7 / 500 [                                                  ] Loss: 0.80707  Patience: 10\n",
      "2024-06-11T09:41:47.223905 >   8 / 500 [                                                  ] Loss: 0.80488  Patience: 10\n",
      "2024-06-11T09:41:49.077248 >   9 / 500 [                                                  ] Loss: 0.80251  Patience: 10\n",
      "2024-06-11T09:41:50.895200 >  10 / 500 [=                                                 ] Loss: 0.80171  Patience: 10\n",
      "2024-06-11T09:41:52.705162 >  11 / 500 [=                                                 ] Loss: 0.80012  Patience: 10\n",
      "2024-06-11T09:41:54.592971 >  12 / 500 [=                                                 ] Loss: 0.79860  Patience: 10\n",
      "2024-06-11T09:41:56.341089 >  13 / 500 [=                                                 ] Loss: 0.79770  Patience: 10\n",
      "2024-06-11T09:41:58.115542 >  14 / 500 [=                                                 ] Loss: 0.79694  Patience: 10\n",
      "2024-06-11T09:41:59.864923 >  15 / 500 [=                                                 ] Loss: 0.79648  Patience: 10\n",
      "2024-06-11T09:42:01.952538 >  16 / 500 [=                                                 ] Loss: 0.79616  Patience: 10\n",
      "2024-06-11T09:42:04.387181 >  17 / 500 [=                                                 ] Loss: 0.79560  Patience: 10\n",
      "2024-06-11T09:42:06.528893 >  18 / 500 [=                                                 ] Loss: 0.79515  Patience: 10\n",
      "2024-06-11T09:42:08.298597 >  19 / 500 [=                                                 ] Loss: 0.79488  Patience: 10\n",
      "2024-06-11T09:42:10.367777 >  20 / 500 [==                                                ] Loss: 0.79448  Patience: 10\n",
      "2024-06-11T09:42:12.173091 >  21 / 500 [==                                                ] Loss: 0.79418  Patience: 10\n",
      "2024-06-11T09:42:13.859002 >  22 / 500 [==                                                ] Loss: 0.79376  Patience: 10\n",
      "2024-06-11T09:42:15.851427 >  23 / 500 [==                                                ] Loss: 0.79359  Patience: 10\n",
      "2024-06-11T09:42:17.424676 >  24 / 500 [==                                                ] Loss: 0.79329  Patience: 10\n",
      "2024-06-11T09:42:19.120586 >  25 / 500 [==                                                ] Loss: 0.79302  Patience: 10\n",
      "2024-06-11T09:42:20.841273 >  26 / 500 [==                                                ] Loss: 0.79282  Patience: 10\n",
      "2024-06-11T09:42:22.508869 >  27 / 500 [==                                                ] Loss: 0.79257  Patience: 10\n",
      "2024-06-11T09:42:24.164467 >  28 / 500 [==                                                ] Loss: 0.79236  Patience: 10\n",
      "2024-06-11T09:42:25.795452 >  29 / 500 [==                                                ] Loss: 0.79216  Patience: 10\n",
      "2024-06-11T09:42:27.385577 >  30 / 500 [===                                               ] Loss: 0.79206  Patience: 10\n",
      "2024-06-11T09:42:28.870027 >  31 / 500 [===                                               ] Loss: 0.79190  Patience: 10\n",
      "2024-06-11T09:42:30.501730 >  32 / 500 [===                                               ] Loss: 0.79178  Patience: 10\n",
      "2024-06-11T09:42:32.086123 >  33 / 500 [===                                               ] Loss: 0.79158  Patience: 10\n",
      "2024-06-11T09:42:33.623010 >  34 / 500 [===                                               ] Loss: 0.79144  Patience: 10\n",
      "2024-06-11T09:42:35.175510 >  35 / 500 [===                                               ] Loss: 0.79123  Patience: 10\n",
      "2024-06-11T09:42:36.883671 >  36 / 500 [===                                               ] Loss: 0.79116  Patience: 10\n",
      "2024-06-11T09:42:38.517801 >  37 / 500 [===                                               ] Loss: 0.79093  Patience: 10\n",
      "2024-06-11T09:42:40.159310 >  38 / 500 [===                                               ] Loss: 0.79078  Patience: 10\n",
      "2024-06-11T09:42:41.745856 >  39 / 500 [===                                               ] Loss: 0.79059  Patience: 10\n",
      "2024-06-11T09:42:43.295550 >  40 / 500 [====                                              ] Loss: 0.79035  Patience: 10\n",
      "2024-06-11T09:42:44.818821 >  41 / 500 [====                                              ] Loss: 0.79021  Patience: 10\n",
      "2024-06-11T09:42:46.382644 >  42 / 500 [====                                              ] Loss: 0.78989  Patience: 10\n",
      "2024-06-11T09:42:47.956976 >  43 / 500 [====                                              ] Loss: 0.78968  Patience: 10\n",
      "2024-06-11T09:42:49.543463 >  44 / 500 [====                                              ] Loss: 0.78961  Patience: 10\n",
      "2024-06-11T09:42:51.126396 >  45 / 500 [====                                              ] Loss: 0.78943  Patience: 10\n",
      "2024-06-11T09:42:52.645048 >  46 / 500 [====                                              ] Loss: 0.78926  Patience: 10\n",
      "2024-06-11T09:42:54.205172 >  47 / 500 [====                                              ] Loss: 0.78929  Patience: 9\n",
      "2024-06-11T09:42:55.745868 >  48 / 500 [====                                              ] Loss: 0.78908  Patience: 9\n",
      "2024-06-11T09:42:57.277127 >  49 / 500 [====                                              ] Loss: 0.78903  Patience: 9\n",
      "2024-06-11T09:42:58.786501 >  50 / 500 [=====                                             ] Loss: 0.78892  Patience: 9\n",
      "2024-06-11T09:43:00.361654 >  51 / 500 [=====                                             ] Loss: 0.78882  Patience: 9\n",
      "2024-06-11T09:43:01.901411 >  52 / 500 [=====                                             ] Loss: 0.78881  Patience: 9\n",
      "2024-06-11T09:43:03.566314 >  53 / 500 [=====                                             ] Loss: 0.78865  Patience: 9\n",
      "2024-06-11T09:43:05.093131 >  54 / 500 [=====                                             ] Loss: 0.78859  Patience: 9\n",
      "2024-06-11T09:43:06.638659 >  55 / 500 [=====                                             ] Loss: 0.78853  Patience: 9\n",
      "2024-06-11T09:43:08.169715 >  56 / 500 [=====                                             ] Loss: 0.78845  Patience: 9\n",
      "2024-06-11T09:43:09.741391 >  57 / 500 [=====                                             ] Loss: 0.78841  Patience: 9\n",
      "2024-06-11T09:43:11.284760 >  58 / 500 [=====                                             ] Loss: 0.78836  Patience: 9\n",
      "2024-06-11T09:43:12.852256 >  59 / 500 [=====                                             ] Loss: 0.78832  Patience: 9\n",
      "2024-06-11T09:43:14.390693 >  60 / 500 [======                                            ] Loss: 0.78834  Patience: 8\n",
      "2024-06-11T09:43:15.994896 >  61 / 500 [======                                            ] Loss: 0.78836  Patience: 7\n",
      "2024-06-11T09:43:17.591938 >  62 / 500 [======                                            ] Loss: 0.78825  Patience: 7\n",
      "2024-06-11T09:43:19.179532 >  63 / 500 [======                                            ] Loss: 0.78812  Patience: 7\n",
      "2024-06-11T09:43:20.813066 >  64 / 500 [======                                            ] Loss: 0.78806  Patience: 7\n",
      "2024-06-11T09:43:22.397727 >  65 / 500 [======                                            ] Loss: 0.78811  Patience: 6\n",
      "2024-06-11T09:43:24.124197 >  66 / 500 [======                                            ] Loss: 0.78795  Patience: 6\n",
      "2024-06-11T09:43:25.783906 >  67 / 500 [======                                            ] Loss: 0.78787  Patience: 6\n",
      "2024-06-11T09:43:27.403832 >  68 / 500 [======                                            ] Loss: 0.78785  Patience: 6\n",
      "2024-06-11T09:43:29.003355 >  69 / 500 [======                                            ] Loss: 0.78780  Patience: 6\n",
      "2024-06-11T09:43:30.592092 >  70 / 500 [=======                                           ] Loss: 0.78772  Patience: 6\n",
      "2024-06-11T09:43:32.191588 >  71 / 500 [=======                                           ] Loss: 0.78770  Patience: 6\n",
      "2024-06-11T09:43:33.735105 >  72 / 500 [=======                                           ] Loss: 0.78770  Patience: 5\n",
      "2024-06-11T09:43:35.282264 >  73 / 500 [=======                                           ] Loss: 0.78756  Patience: 5\n",
      "2024-06-11T09:43:36.824152 >  74 / 500 [=======                                           ] Loss: 0.78752  Patience: 5\n",
      "2024-06-11T09:43:38.384244 >  75 / 500 [=======                                           ] Loss: 0.78741  Patience: 5\n",
      "2024-06-11T09:43:39.934133 >  76 / 500 [=======                                           ] Loss: 0.78747  Patience: 4\n",
      "2024-06-11T09:43:41.482949 >  77 / 500 [=======                                           ] Loss: 0.78732  Patience: 4\n",
      "2024-06-11T09:43:43.036529 >  78 / 500 [=======                                           ] Loss: 0.78739  Patience: 3\n",
      "2024-06-11T09:43:44.613047 >  79 / 500 [=======                                           ] Loss: 0.78732  Patience: 2\n",
      "2024-06-11T09:43:46.078732 >  80 / 500 [========                                          ] Loss: 0.78732  Patience: 1\n",
      "2024-06-11T09:43:47.600085 >  81 / 500 [========                                          ] Loss: 0.78722  Patience: 1\n",
      "2024-06-11T09:43:49.324079 >  82 / 500 [========                                          ] Loss: 0.78716  Patience: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "refiner_train_data, refiner_validation_data, refiner_test_data = format_refiner_data()\n",
    "\n",
    "N_MAX_EPOCHS = 500\n",
    "LSR = deep.NN(\n",
    "    model=cnn.UNet2DRefiner(SUBDIVISION_SIZE, 3),\n",
    "    n_epochs=N_MAX_EPOCHS,\n",
    "    optimizer=optim.Adam,\n",
    "    loss_fn=nn.CrossEntropyLoss,\n",
    "    use_half=False,\n",
    "    patience=10,\n",
    "    verbosity=2,\n",
    "    batch=256\n",
    ")\n",
    "LSR.train(refiner_train_data, refiner_validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 scores: [0.11428571492433548, 0.37288135290145874, 0.47457626461982727, 0.222222238779068, 0.0, 0.2631579041481018, 0.0, 0.0555555559694767, 0.2142857164144516, 0.2181818187236786, 0.1875, 0.23255814611911774, 0.2631579041481018, 0.3050847351551056, 0.36000001430511475, 0.2702702581882477, 0.3225806653499603, 0.38596490025520325, 0.23529411852359772, 0.4150943458080292, 0.17543861269950867, 0.06666666269302368, 0.2926829159259796, 0.19607841968536377, 0.0, 0.5517241358757019, 0.38596490025520325, 0.5283018946647644, 0.23529411852359772, 0.2448979616165161, 0.2916666567325592, 0.21052633225917816, 0.0, 0.4210526645183563, 0.21052631735801697, 0.0, 0.12903225421905518, 0.24137932062149048, 0.33898305892944336, 0.2916666567325592, 0.1690140962600708, 0.3333333134651184, 0.0784313753247261, 0.3870967924594879, 0.0, 0.10909091681241989, 0.0, 0.27272725105285645, 0.13333332538604736, 0.40909093618392944, 0.12765957415103912, 0.0, 0.2352941334247589, 0.12244898080825806, 0.05128205195069313, 0.3199999928474426, 0.3921568989753723, 0.2641509473323822, 0.1355932205915451, 0.16326531767845154, 0.3529411554336548, 0.37931036949157715, 0.3333333432674408, 0.2181818187236786, 0.0, 0.5294117331504822, 0.3214285671710968, 0.19230769574642181, 0.0, 0.444444477558136, 0.517241358757019, 0.5161290168762207, 0.23076923191547394, 0.20000000298023224, 0.23255811631679535, 0.49180328845977783, 0.10169492661952972, 0.05714285746216774, 0.0, 0.0, 0.222222238779068, 0.0, 0.0, 0.0, 0.43478259444236755, 0.0, 0.146341472864151, 0.3125000298023224, 0.0, 0.0, 0.3999999761581421, 0.3396226465702057, 0.042553193867206573, 0.3076923191547394, 0.15789474546909332, 0.0, 0.0, 0.043478261679410934, 0.3636363744735718, 0.0, 0.5084745287895203, 0.11764706671237946, 0.17647060751914978, 0.0, 0.2545454502105713, 0.32258063554763794, 0.3199999928474426, 0.04000000283122063, 0.24000000953674316, 0.04651162773370743, 0.3181818425655365, 0.18518517911434174, 0.2666666507720947, 0.4074074327945709, 0.5762712359428406, 0.2800000011920929, 0.21739129722118378, 0.20588235557079315, 0.22857142984867096, 0.36666667461395264, 0.0, 0.25925925374031067, 0.11320754885673523, 0.1600000113248825, 0.1666666716337204, 0.3921568691730499, 0.4516129195690155, 0.21276594698429108, 0.40625, 0.42307692766189575, 0.4406779706478119, 0.4615384638309479, 0.08163265883922577, 0.1600000113248825, 0.0, 0.3461538255214691, 0.13636364042758942, 0.20408163964748383, 0.21276596188545227, 0.3272727429866791, 0.19230769574642181, 0.0, 0.1600000113248825, 0.0, 0.24000002443790436, 0.1600000113248825, 0.29411762952804565, 0.21621622145175934, 0.4117647111415863, 0.09090909361839294, 0.19999998807907104, 0.19354838132858276, 0.2222222238779068, 0.4363636374473572, 0.07407408207654953, 0.1600000113248825, 0.27586206793785095, 0.1666666716337204, 0.2083333432674408, 0.320000022649765, 0.222222238779068, 0.15789476037025452, 0.27027028799057007, 0.21276594698429108, 0.5084745287895203, 0.06896551698446274, 0.1666666716337204, 0.5084745287895203, 0.4126984179019928, 0.4126984179019928, 0.27272728085517883, 0.2857142686843872, 0.29411762952804565, 0.4363636374473572, 0.2909091114997864, 0.1538461446762085, 0.0, 0.4482758343219757, 0.0, 0.11999998986721039, 0.1875, 0.0, 0.28125, 0.20512820780277252, 0.06451612710952759, 0.2448979616165161, 0.0, 0.4193548262119293, 0.05714285746216774, 0.2916666865348816, 0.2978723347187042, 0.0, 0.20408163964748383, 0.5263158082962036, 0.1621621698141098, 0.17142857611179352, 0.0, 0.1428571343421936, 0.3333333432674408, 0.0, 0.0, 0.0714285746216774, 0.5333333015441895, 0.1904761791229248, 0.12765958905220032, 0.05128205195069313, 0.0, 0.22727273404598236, 0.21052631735801697, 0.0, 0.11538461595773697, 0.1600000113248825, 0.5098038911819458, 0.09090909361839294, 0.3750000298023224, 0.4137931168079376, 0.5490196347236633, 0.4444444477558136, 0.2857142984867096, 0.3199999928474426, 0.4642857313156128, 0.3050847351551056, 0.4444444477558136, 0.3214285671710968, 0.2068965584039688, 0.11538461595773697, 0.05263157933950424, 0.23728813230991364, 0.1428571492433548, 0.21276596188545227, 0.033898305147886276, 0.10810811072587967]\n",
      "Arithmetic mean: 0.2221032890073698\n"
     ]
    }
   ],
   "source": [
    "def model_4(pp: np.ndarray, mask: np.ndarray):\n",
    "    x = Estimator.predict([pp])\n",
    "    y = refine(x, pp, LSR)\n",
    "    x = y\n",
    "    x = OU(x, mask)\n",
    "    return x\n",
    "\n",
    "\n",
    "model_4_results = measure_performances(model_4, test_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
